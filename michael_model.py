# -*- coding: utf-8 -*-
"""michael_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wRSBQ79uPdCXEOgyWmO3tlic5UoG-Mgm
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import ttest_ind
import pandas_profiling as pp
import joblib
from sklearn.preprocessing import StandardScaler
import sklearn.metrics as metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import r2_score
from sklearn.metrics import mean_squared_error
from sklearn.metrics import explained_variance_score
from sklearn.model_selection import train_test_split
#from lightgbm import LGBMClassifier
import matplotlib
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import plot_confusion_matrix
from sklearn.model_selection import learning_curve
from sklearn.model_selection import validation_curve
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import GridSearchCV

from google.colab import drive

drive.mount('/content/gdrive')
root_path = "gdrive/My Drive/JPMC_Hackathon/"
data_path = root_path + "data/"

df = pd.read_csv(data_path+"df_imputed.csv")

df = df.drop(['Unnamed: 0'],axis=1)

df

"""# Data Augmentation
## Step 1. Linear Interpolation (not ideal but while we wait for other data to process this is what we will do to run our models)
"""

X = df.drop(['GeoAreaName','Time_Detail'],axis=1)



dataColnames = X.columns

dataColnames

str(2000+yearScalar)



df_allCountryNewData = pd.DataFrame(columns=df.columns)

for country in df['GeoAreaName']:
  df_country = df[df['GeoAreaName']==country]
  if len(df_country) > 1:
    minYear = min(df_country['Time_Detail'])
    maxYear = max(df_country['Time_Detail'])
    print(country,minYear,maxYear)
    minYearVals = df_country[df_country['Time_Detail'] == minYear].drop(['GeoAreaName','Time_Detail'],axis=1).to_numpy()
    maxYearVals = df_country[df_country['Time_Detail'] == maxYear].drop(['GeoAreaName','Time_Detail'],axis=1).to_numpy()
    linearInterp = (maxYearVals-minYearVals) / (int(maxYear) - int(minYear))
    print(country,minYear,maxYear,linearInterp)
    baseYear2000Vals = minYearVals - (minYear - 2000) * linearInterp
    newYearDataList = []
    for yearScalar in range(20):
      d = pd.DataFrame.from_dict({'GeoAreaName':[country],"Time_Detail":[str(2000+yearScalar)]})
      newYearData = baseYear2000Vals + yearScalar * linearInterp
      df_tempData = pd.DataFrame(data=newYearData,columns=dataColnames)
      df_newYearData = pd.concat([d, df_tempData], axis=1)
      print(df_newYearData)
      df_allCountryNewData=df_allCountryNewData.append(df_newYearData) 
  
  # not using countries with only one data point
  #else:
  #  minYear = min(df_country['Time_Detail'])
  #  #maxYear = max(df_country['Time_Detail'])
  #  #print(country,minYear,maxYear)
  #  minYearVals = df_country[df_country['Time_Detail'] == minYear].drop(['GeoAreaName','Time_Detail'],axis=1).to_numpy()
  #  #maxYearVals = df_country[df_country['Time_Detail'] == maxYear].drop(['GeoAreaName','Time_Detail'],axis=1).to_numpy()
  #  for yearScalar in range(19):
  #    d = pd.DataFrame({'GeoAreaName':[country],"Time_Detail":[str(2000+yearScalar)]})
  #    newYearData = minYearVals
  #    df_tempData = pd.DataFrame(data=newYearData,columns=dataColnames)
  #    df_newYearData = pd.concat([d, df_tempData], axis=1)
  #    print(df_newYearData)
  #    df_allCountryNewData=df_allCountryNewData.append(df_newYearData)

df_allCountryNewData



"""## Analyses we can do
1. Proportion of children and young people achieving a minimum proficiency level in reading and mathematics (%)
2. **Completion rate, by sex, location, wealth quintile and education level (%)
bold text**
3. Proportion of children aged 36âˆ’59 months who are developmentally on track in at least three of the following domains: literacy-numeracy, physical development, social-emotional development, and learning (% of children aged 36-59 months)
4. **Participation rate in organized learning (one year before the official primary entry age), by sex (%)**
5. **Participation rate in formal and non-formal education and training, by sex (%)**
6. Proportion of youth and adults with information and communications technology (ICT) skills, by sex and type of skill (%)
7. **Adjusted gender parity index for completion rate, by sex, location, wealth quintile and education level**
8. Adjusted location parity index for completion rate, by sex, location, wealth quintile and education level
9. Adjusted wealth parity index for completion rate, by sex, location, wealth quintile and education level
10. Gender parity index for youth/adults with information and communications technology (ICT) skills, by type of skill (ratio)
11. **Gender parity index for participation rate in formal and non-formal education and training (ratio)**
12. **Gender parity index for participation rate in organized learning (one year before the official primary entry age), (ratio)**
13. **Gender parity index of trained teachers, by education level (ratio)**
14. Immigration status parity index for achieving at least a fixed level of proficiency in functional skills, by numeracy/literacy skills (ratio)
15. Language test parity index for achievement (ratio)
16. Native parity index for achievement (ratio)
17. **Gender parity index for achievement (ratio)**
18. Rural to urban parity index for achievement (ratio)
19. Low to high socio-economic parity status index for achievement (ratio)
20. Proportion of population achieving at least a fixed level of proficiency in functional skills, by sex, age and type of skill (%)
21. Schools with basic handwashing facilities, by education level (%)
22. Schools with access to\xa0computers for pedagogical purposes, by education level (%)
23. Schools with access to\xa0electricity, by education level (%)
24. Schools with access to basic drinking water, by education level (%)
25. Schools with access to the internet for pedagogical purposes, by education level (%)
26. Schools with access to\xa0access to single-sex basic sanitation, by education level (%)
27. Proportion of schools with access to adapted infrastructure and materials for students with disabilities, by education level (%)
28. Total official flows for scholarships, by recipient countries (millions of constant 2018 United States dollars)
29. Proportion of teachers who have received at least the minimum organized teacher training (e.g. pedagogical training) pre-service or in-service required for teaching at the relevant level in a given country, by sex and education level (%)


"""

df_allCountryNewData

df_allCountryNewData.columns

df_allCountryNewData.to_csv(data_path+"df_allCountryNewData.csv")

df_allCountryNewData

targetsList = ['BOTHSEX.SKILL_READ.GRAD23',
       'BOTHSEX.SKILL_READ.LOWSEC', 'FEMALE.SKILL_MATH.GRAD23',
       'FEMALE.SKILL_MATH.LOWSEC', 'MALE.SKILL_MATH.GRAD23',
       'BOTHSEX.SKILL_MATH.PRIMAR', 'MALE.SKILL_READ.GRAD23',
       'MALE.SKILL_MATH.LOWSEC', 'FEMALE.SKILL_READ.LOWSEC',
       'BOTHSEX.SKILL_READ.PRIMAR', 'FEMALE.SKILL_READ.GRAD23',
       'BOTHSEX.SKILL_MATH.LOWSEC', 'MALE.SKILL_READ.PRIMAR',
       'FEMALE.SKILL_READ.PRIMAR', 'BOTHSEX.SKILL_MATH.GRAD23',
       'MALE.SKILL_MATH.PRIMAR', 'MALE.SKILL_READ.LOWSEC',
       'FEMALE.SKILL_MATH.PRIMAR']

targetsList

df_allCountryNewData['GeoAreaName'].unique()

df_allCountryNewData

df_Colombia = df_allCountryNewData[df_allCountryNewData['GeoAreaName']=='Colombia'].drop(['GeoAreaName','Time_Detail'],axis=1)[:20]

df_CostaRica = df_allCountryNewData[df_allCountryNewData['GeoAreaName']=='Costa Rica'].drop(['GeoAreaName','Time_Detail'],axis=1)[:20]

df_Guatemala = df_allCountryNewData[df_allCountryNewData['GeoAreaName']=='Guatemala'].drop(['GeoAreaName','Time_Detail'],axis=1)[:20]

df_Mexico = df_allCountryNewData[df_allCountryNewData['GeoAreaName']=='Mexico'].drop(['GeoAreaName','Time_Detail'],axis=1)[:20]

df_Colombia

df_CostaRica

df_Guatemala

df_Mexico

from statsmodels.tsa.vector_ar.var_model import VAR
from random import random
# contrived dataset with dependency
data = list()
for i in range(100):
    v1 = i + random()
    v2 = v1 + random()
    row = [v1, v2]
    data.append(row)
# fit model
model = VAR(data)
model_fit = model.fit()
# make prediction
yhat = model_fit.forecast(model_fit.y, steps=1)
print(yhat)

from statsmodels.tsa.vector_ar.var_model import VAR
# fit model
model_Columbia = VAR(data)
model_fit_Columbia = model_Columbia.fit()
print(model_fit_Columbia.summary())
# make prediction
yhat_Columbia = model_fit_Columbia.forecast(model_fit_Columbia.y, steps=1)
print(yhat_Columbia)

from statsmodels.tsa.vector_ar.var_model import VAR
# fit model
model_CostaRica = VAR(data)
model_fit_CostaRica = model_CostaRica.fit()
print(model_fit_CostaRica.summary())
# make prediction
yhat_CostaRica = model_fit_CostaRica.forecast(model_fit_CostaRica.y, steps=1)
print(yhat_CostaRica)

from statsmodels.tsa.vector_ar.var_model import VAR
# fit model
model_Guatemala = VAR(data)
model_fit_Guatemala = model_Guatemala.fit()
print(model_fit_Guatemala.summary())
# make prediction
yhat_Guatemala = model_fit_Guatemala.forecast(model_fit_Guatemala.y, steps=1)
print(yhat_Guatemala)

from statsmodels.tsa.vector_ar.var_model import VAR
# fit model
model_Mexico = VAR(data)
model_fit_Mexico = model_Mexico.fit()
print(model_fit_Mexico.summary())
# make prediction
yhat_Mexico = model_fit_Mexico.forecast(model_fit_Mexico.y, steps=1)
print(yhat_Mexico)



x

